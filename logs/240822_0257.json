[
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/consequences_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            78
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/im_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            98,
            50,
            40,
            75,
            77,
            78
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/is_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            6,
            9,
            12,
            14,
            25,
            27,
            32,
            33,
            34,
            36,
            37,
            38,
            39,
            40,
            41,
            43,
            46,
            47,
            49,
            51,
            52,
            53,
            57,
            58,
            59,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            71,
            72,
            76,
            78,
            79,
            83,
            84,
            87,
            92,
            93,
            96,
            97
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/js_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            88
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/situation_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            2,
            4,
            6,
            10,
            81,
            18,
            55,
            24,
            30
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/unusual_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            0,
            1,
            2,
            4,
            5,
            6,
            8,
            9,
            10,
            11,
            12,
            13,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            29,
            30,
            31,
            32,
            33,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            67,
            68,
            69,
            71,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99
        ]
    },
    {
        "config": {
            "model_name": "llava-mistral",
            "task_type": "tasks/common_problem_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            31
        ]
    },
    {
        "config": {
            "model_name": "llava-mistral",
            "task_type": "tasks/consequences_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            65
        ]
    },
    {
        "config": {
            "model_name": "llava-mistral",
            "task_type": "tasks/im_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            67,
            54,
            42,
            75,
            31,
            63,
            47
        ]
    },
    {
        "config": {
            "model_name": "llava-mistral",
            "task_type": "tasks/is_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            62
        ]
    },
    {
        "config": {
            "model_name": "llava-mistral",
            "task_type": "tasks/js_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            56,
            45,
            29
        ]
    },
    {
        "config": {
            "model_name": "llava-mistral",
            "task_type": "tasks/situation_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            24,
            62,
            15
        ]
    },
    {
        "config": {
            "model_name": "llava-mistral",
            "task_type": "tasks/unusual_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            11
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/common_problem_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            75
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/consequences_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            64,
            68,
            42,
            43,
            46,
            18,
            19,
            89
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/im_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            33,
            22,
            25,
            31,
            77,
            15
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/is_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            59,
            78
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/js_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            4,
            6,
            9,
            10,
            13,
            16,
            27,
            28,
            32,
            35,
            38,
            44,
            45,
            46,
            48,
            49,
            52,
            55,
            58,
            59,
            60,
            63,
            64,
            67,
            70,
            72,
            80,
            85,
            91,
            93,
            95,
            96,
            99
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/situation_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            2,
            6,
            72,
            15,
            79,
            87,
            23,
            56
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/unusual_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            22
        ]
    }
]