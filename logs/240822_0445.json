[
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/common_problem_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            58,
            28,
            5,
            30
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/consequences_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            93
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/im_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            96,
            33,
            32,
            35,
            69,
            39,
            41,
            75,
            15,
            51,
            87,
            95
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/is_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            21,
            23,
            24,
            26,
            27,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            48,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            62,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            96,
            97,
            98,
            99
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/js_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            1,
            37,
            6,
            39,
            9,
            41,
            76,
            81,
            85,
            23,
            88
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/situation_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            70,
            7,
            71,
            72,
            76,
            79,
            17,
            84,
            86,
            23,
            90,
            29,
            95,
            33,
            34,
            37,
            40,
            45,
            47,
            52,
            62,
            63
        ]
    },
    {
        "config": {
            "model_name": "llava-llama3",
            "task_type": "tasks/unusual_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            1,
            65,
            5,
            71,
            7,
            11,
            77,
            17,
            19,
            88,
            34,
            99,
            45,
            50,
            52,
            59,
            60
        ]
    },
    {
        "config": {
            "model_name": "llava-mistral",
            "task_type": "tasks/is_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            92,
            15,
            39
        ]
    },
    {
        "config": {
            "model_name": "llava-mistral",
            "task_type": "tasks/unusual_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            11
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/consequences_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            64,
            18,
            68,
            89,
            42,
            43,
            46
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/im_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            33,
            68,
            69,
            90,
            77,
            15,
            16,
            79,
            81,
            88,
            58,
            60,
            31
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/is_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            0,
            1,
            3,
            5,
            8,
            10,
            14,
            19,
            23,
            24,
            28,
            29,
            30,
            32,
            35,
            36,
            39,
            41,
            42,
            43,
            44,
            47,
            49,
            52,
            53,
            56,
            58,
            59,
            61,
            63,
            66,
            67,
            71,
            73,
            74,
            75,
            76,
            77,
            78,
            84,
            86,
            88,
            95,
            96,
            97,
            99
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/js_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            64,
            67,
            4,
            6,
            9,
            10,
            16,
            80,
            85,
            25,
            27,
            91,
            93,
            95,
            32,
            96,
            97,
            35,
            41,
            44,
            45,
            46,
            48,
            49,
            50,
            51,
            52,
            55,
            58,
            59,
            63
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/situation_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            3,
            6,
            72,
            79,
            87,
            23,
            56,
            93
        ]
    },
    {
        "config": {
            "model_name": "llava-vicuna",
            "task_type": "tasks/unusual_task_prompt.json",
            "stimuli": {
                "name": "base",
                "text": ""
            },
            "overwrite": false,
            "example_num": 100,
            "generate_answer_num": 5,
            "batch_size": 1,
            "lang": "ko"
        },
        "error_queries": [
            22
        ]
    }
]